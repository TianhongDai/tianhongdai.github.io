<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155128387-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-155128387-1');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tianhong Dai</title>
  <meta name="author" content="Tianhong Dai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <div class="container">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianhong Dai</name>
              </p>
              <p style="text-align:justify">I am a second year PhD candidate in Machine Learning at Imperial College London, under the supervision of <a href="https://www.imperial.ac.uk/people/a.bharath">Prof. Anil Bharath</a>. My current research interests are Deep Reinforcement Learning, Machine Learning and Computer Vision. 
              </p>
              <p style="text-align:justify">
                In my spare time, I like playing basketball, playing video games, watching movies, coding and implementing state-of-the-art algorithms. I also like investigating digital circuits.
              </p>
              <p style="text-align:center">
                <a href="mailto:tianhong.dai15@imperial.ac.uk">Email</a> &nbsp/&nbsp
                <a href="data/Tianhong_Dai_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/TianhongDai">GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=9apiamkAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/tianhong-dai-71b166117/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/picture_td.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/picture_td.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
              <p>
                <ul>
                <li>
                  <div>PhD in Machine Learning, Imperial College London</div>
                  <div style="float: right; text-align: right; margin: -17px"><em>Oct. 2017 - Apr. 2022 (Expected)</em></div>
                </li>
                <li style="line-height:200%">
                  <div>MSc in Communication and Signal Processing, Imperial College London</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 11px"><em>Oct. 2015 - Sep. 2016</em></div>
                </li>
                <li>
                  <div>BEng in Electronic and Communication Engineering, University of Liverpool</div>
                  <div style="float: right; text-align: right; margin: -17px"><em>Sep. 2013 - Jun. 2015</em></div>
                </li>
                <li style="line-height:200%">
                  <div>BEng in Electronic and Communication Engineering, Xi'an Jiaotong Liverpool University</div> 
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 11px"><em>Sep. 2011 - Jun. 2013</em></div>
                </li> 
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Working Experience / Activities</heading>
              <p>
                <ul>
                <li>
                  <div>Research Intern (advised by <a href="https://mengf1.github.io/">Dr. Meng Fang</a>) in Tencent AI Lab / Robotics X (Shenzhen, China)</div>
                  <div style="float: right; text-align: right; margin: -17px"><em>Mar. 2019 - Sep. 2019</em></div>
                </li>
                <li style="line-height:200%">
                  <div>Invited to give a PyTorch Tutorial in Alan Turing Institute (London, United Kingdom)</div>
                  <div style="float: right; text-align: right; margin: -28px; padding-right: 11px"><em>Aug. 2018</em></div>
                </li>
                <li>
                  <div>Summer Intern in Perkins Shibaura Engines (Wuxi) Co., Ltd (Wuxi, China)</div>
                  <div style="float: right; text-align: right; margin: -17px"><em>July. 2014 - Aug. 2014</em></div>
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Publications</heading> (<sup>*</sup> Equal Contributions)
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- # paper: Analysing Deep Reinforcement Learning Agents Trained with Domain Randomisation -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/interp.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.08324.pdf">
                <papertitle>Analysing Deep Reinforcement Learning Agents Trained with Domain Randomisation</papertitle>
              </a>
              <br>
              <strong>Tianhong Dai<sup>*</sup></strong>,
              Kai Arulkumaran<sup>*</sup>,
              Samyakh Tukra,
              Feryal Behbahani,
              Anil Anthony Bharath
              <br>
              <em>arxiv Preprint</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1912.08324.pdf">pdf</a> /
              <a href="data/interp.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">Domain randomisation is one of the most popluar method which can train agents in simulation, and then transfer them to the real world. However, less work has gone into understanding such agents. In this work we examine such agents, through qualitative and quantitative comparisons between agents trained with and without visual domain randomisation, in order to provide a better understanding of how they function.</p>
            </td>
          </tr>

          <!-- # paper: Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gated_detection.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1910.11761.pdf">
                <papertitle>Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection</papertitle>
              </a>
              <br>
              Tianrui Liu,
              Jun-Jie Huang,
              <strong>Tianhong Dai</strong>,
              Guangyu Ren,
              Tania Stathaki
              <br>
              <em>arxiv Preprint</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1910.11761.pdf">pdf</a> /
              <a href="data/gated_detection.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify"> In this paper, we propose a gated multi-layer convolutional feature extraction method which can adaptively generate discriminative features for candidate pedestrian regions.</p>
            </td>
          </tr>

          <!-- # paper: LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/liir.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning.pdf">
                <papertitle>LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning</papertitle>
              </a>
              <br>
              Yali Du<sup>*</sup>,
              Lei Han<sup>*</sup>,
              Meng Fang,
              Ji Liu,
              <strong>Tianhong Dai</strong>,
              Dacheng Tao
              <br>
              <em>33rd Conference on Neural Information Processing Systems (NeurIPS)</em>, 2019
              <br>
              <a href="http://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning.pdf">pdf</a> /
              <a href="data/liir.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We have proposed a novel multi-agent reinforcement learning algorithm, which learns an individual intrinsic reward for each agent. The method can assign each agent a distinct intrinsic reward so that the agents are stimulated differently, even when the environment only feedbacks a team reward.</p>
            </td>
          </tr>

          <!-- # paper: A Maximum Entropy Deep ReinforcementLearning Neural Tracker -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/max_entropy.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-32692-0_46">
                <papertitle>A Maximum Entropy Deep ReinforcementLearning Neural Tracker</papertitle>
              </a>
              <br>
              Shafa Balaram,
              Kai Arulkumaran,
              <strong>Tianhong Dai</strong>,
              Anil Anthony Bharath
              <br>
              <em>International Workshop on Machine Learning in Medical Imaging (MLMI)</em>, 2019
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-32692-0_46">pdf</a> /
              <a href="data/max_ent_tracker.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We introducea maximum entropy continuous-action DRL neural tracker capable oftraining from scratch in a complex environment in the presence of highnoise levels, Gaussian blurring and cell detractors.</p>
            </td>
          </tr>

          <!-- # paper: Image Synthesis with a Convolutional Capsule Generative Adversarial Network -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/image_synthesis.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v102/bass19a/bass19a.pdf">
                <papertitle>Image Synthesis with a Convolutional Capsule Generative Adversarial Network</papertitle>
              </a>
              <br>
              Cher Bass,
              <strong>Tianhong Dai</strong>,
              Benjamin Billot,
              Kai Arulkumaran,
              Antonia Creswell,
              Claudia Clopath,
              Vincenzo De Paola,
              Anil Anthony Bharath
              <br>
              <em>International Conference on Medical Imaging with Deep Learning (MIDL)</em>, 2019
              <br>
              <a href="http://proceedings.mlr.press/v102/bass19a/bass19a.pdf">pdf</a> /
              <a href="data/image_synthesis.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We introduce CapsPix2Pix, which combines convolutional capsules with the pix2pix framework, to synthesise images conditioned on class segmentation labels. We apply our approach to a new biomedical dataset of cortical axons imaged by two-photon microscopy, as a method of data augmentation for small datasets.</p>
            </td>
          </tr>

          <!-- # paper: Deep Reinforcement Learning for Subpixel Neural Tracking -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/axon_tracking.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v102/dai19a/dai19a.pdf">
                <papertitle>Deep Reinforcement Learning for Subpixel Neural Tracking</papertitle>
              </a>
              <br>
              <strong>Tianhong Dai</strong>,
              Magda Dubois,
              Kai Arulkumaran,
              Jonathan Campbell,
              Cher Bass,
              Benjamin Billot,
              Fatmatulzehra Uslu,
              Vincenzo de Paola,
              Claudia Clopath,
              Anil Anthony Bharath
              <br>
              <em>International Conference on Medical Imaging with Deep Learning (MIDL)</em>, 2019
              <br>
              <a href="http://proceedings.mlr.press/v102/dai19a/dai19a.pdf">pdf</a> /
              <a href="data/axon_tracking.bib">bibtex</a>
              <br>
              <p></p>
              <p style="text-align:justify">We formulate tracking as a reinforcement learning problem, and apply deep reinforcement learning techniques with a continuous action space to learn how to track at the subpixel level.</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Open Source</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- # paper: Analysing Deep Reinforcement Learning Agents Trained with Domain Randomisation -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/rl_logo.png' height="100%" width="100%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/TianhongDai/reinforcement-learning-algorithms">
                <papertitle>RL-Playground</papertitle>
              </a>
              <br>
              <strong>Tianhong Dai</strong>
              <br>
              <em>GitHub</em>, 2018
              <br>
              <a href="https://github.com/TianhongDai/reinforcement-learning-algorithms">code</a>
              <br>
              <p></p>
              <p style="text-align:justify">This repository will implement the classic deep reinforcement learning algorithms by using PyTorch. The aim of this repository is to provide clear code for people to learn the deep reinforcemen learning algorithms. In the future, more algorithms will be added and the existing codes will also be maintained.</p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                <hr>
                The original template of this page can be found from <a href="https://github.com/jonbarron/website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</div>
</body>
</html>
